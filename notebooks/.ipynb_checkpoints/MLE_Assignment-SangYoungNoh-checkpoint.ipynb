{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c99ca3d",
   "metadata": {},
   "source": [
    "\n",
    "Author: Sang Young Noh \n",
    "------------------------------------\n",
    "\n",
    "Last Updated: 27/01/2022\n",
    "-----------------------------------\n",
    "------------------------------\n",
    "\n",
    "Tasks:\n",
    "=====\n",
    "\n",
    "- Create a ML model in order to predict product category based on appropriate columns. \n",
    "\n",
    "\n",
    "- Present the result in a vivid way and explain your model from a statistical POV\n",
    "\n",
    "\n",
    "- Create a microservice with a REST-api which obtains an article text and returns the \n",
    "  product category\n",
    "\n",
    "\n",
    "- Convert all the code written in above sub-tasks to production quality code \n",
    "\n",
    "\n",
    "- Commit all code to Git. Share the git repository with the GFK MLE Team members mentioned in the email\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "Column Information\n",
    "----------------------------\n",
    "\n",
    "- Columns: id - A unique record identifier \n",
    "- Product Group: Product category \n",
    "- Main_text: A describing text about the article \n",
    "- Add_text: An additional describing text to the artcile \n",
    "- Manufacturer - The manufactuer belonging to the artcie \n",
    "\n",
    "------------------------------------------------------------------\n",
    "\n",
    "Questions:\n",
    "---------------\n",
    "\n",
    "My first thoughts are that the describing texts for the products listing in the table are \n",
    "muddled - we need to simplify the text so that only the most essential part remains. Hence, we will download a \n",
    "german corpus and remove the stopwords. I will also make use of the bag-of-words approach to numerically change the category column.\n",
    "\n",
    "\n",
    "- How do I measure the importance of each column to the final determination of the productgroup?\n",
    "\n",
    "- How to statiscally quantify that the columns others than the brands are not indicative or helpful in finding our what product it is?\n",
    "\n",
    "\n",
    "My first thoughts are that the describing texts for the products listing in the table are \n",
    "muddled - we need to simplify the text so that only the most essential part remains. Hence, we will download a \n",
    "german corpus and remove the stopwords\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "For the case of which form of featurization I use for the string text, there are two main options that come in mind: \n",
    "\n",
    "- Bag-of-words\n",
    "\n",
    "- Word2Vec \n",
    "\n",
    "As bag-of-words creates a vector representation that may be very large in dimension, and does not contain\n",
    "the context. However, if the dimension of the vector becomes 'managable', by reducing the string representation \n",
    "in each of the columns, this may be feasible for feature engineering. \n",
    "\n",
    "In the end, I used the bag-of-words representation for this quick prototype of the Machine Learning model. The reason was because the consistent vector length necessary for different length string lists that result from the tokenization/removal of string that comes with the data pipeline here cannot easily be resolved with the word2vec model, but is automatically accounted for with the one-hot vectors produced from the bag of words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f5a92cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate python data library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from collections import Counter\n",
    "\n",
    "# Using the nltk library to import common stopwords in English and German,\n",
    "# which will be used to modify the columns data \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopEnglish = stopwords.words('english') # List for English stopwords\n",
    "stopGerman = stopwords.words('german') # List for German stopwords \n",
    "# Capitalize words using list comphrensions - as the majority of string data here are capitalized \n",
    "stopEnglish = [stopwords.upper() for stopwords in stopEnglish]\n",
    "stopGerman = [stopwords.upper() for stopwords in stopGerman]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib \n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "736b7b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>productgroup</th>\n",
       "      <th>main_text</th>\n",
       "      <th>add_text</th>\n",
       "      <th>manufacturer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26229701</td>\n",
       "      <td>WASHINGMACHINES</td>\n",
       "      <td>WAQ284E25</td>\n",
       "      <td>WASCHMASCHINEN</td>\n",
       "      <td>BOSCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16576864</td>\n",
       "      <td>USB MEMORY</td>\n",
       "      <td>LEEF IBRIDGE MOBILE SPEICHERERWEITERUNG FUER I...</td>\n",
       "      <td>PC__1100COMPUTINGMEMORY__1110MEMORYCARDS</td>\n",
       "      <td>LEEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25646138</td>\n",
       "      <td>BICYCLES</td>\n",
       "      <td>HOLLANDRAD DAMEN 28 ZOLL TUSSAUD 3-GAENGE RH 5...</td>\n",
       "      <td>FAHRRAEDER // SPORTFAHRRAEDER</td>\n",
       "      <td>SCHALOW &amp; KROH GMBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19764614</td>\n",
       "      <td>BICYCLES</td>\n",
       "      <td>DAHON SPEED D7 SCHWARZ ? FALTRAD</td>\n",
       "      <td>SPORTS__30000WHEELED__30070BIKES</td>\n",
       "      <td>DAHON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64836708</td>\n",
       "      <td>USB MEMORY</td>\n",
       "      <td>PNY 16GB LEGO USB FLASH DRIVE + LEGO BRIC</td>\n",
       "      <td>COMPONENT __ MEMORY</td>\n",
       "      <td>PNY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>61028716</td>\n",
       "      <td>BICYCLES</td>\n",
       "      <td>TOPSY KINDERRAD 12 1/2 POLARWEISS O O O  419640</td>\n",
       "      <td>H006W0792344__WERKZEUG_AUTO/FAHRRAD_FAHRRAEDER</td>\n",
       "      <td>SI-ZWEIRAD-VERTRIEBS GMBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>37734138</td>\n",
       "      <td>BICYCLES</td>\n",
       "      <td>CREME ECHO SOLO 16-SPEED WHITE</td>\n",
       "      <td>FAHRRAEDER&gt;&gt;RENNRAEDER&gt;&gt;RENNRAEDER</td>\n",
       "      <td>CREME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>17891755</td>\n",
       "      <td>CONTACT LENSES</td>\n",
       "      <td>ACUVUE 1-DAY MOIST TAGESLINSEN WEICH, 30 STUEC...</td>\n",
       "      <td>HEALTH&amp;PERSONALCARE__3100OPTICS__3101SPHERICCO...</td>\n",
       "      <td>JOHNSON &amp; JOHNSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>42298563</td>\n",
       "      <td>BICYCLES</td>\n",
       "      <td>UNIVEGA TERRENO 1.0 HE MATTBLAUGRAU 45 CM</td>\n",
       "      <td>1_7_4</td>\n",
       "      <td>UNIVEGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>26300286</td>\n",
       "      <td>WASHINGMACHINES</td>\n",
       "      <td>LAVAMAT 63479 FL A+++ WASCHVOLLAUTOMAT</td>\n",
       "      <td>WASCHMASCHINEN</td>\n",
       "      <td>AEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6655 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     productgroup  \\\n",
       "0     26229701  WASHINGMACHINES   \n",
       "1     16576864       USB MEMORY   \n",
       "3     25646138         BICYCLES   \n",
       "4     19764614         BICYCLES   \n",
       "5     64836708       USB MEMORY   \n",
       "...        ...              ...   \n",
       "7995  61028716         BICYCLES   \n",
       "7996  37734138         BICYCLES   \n",
       "7997  17891755   CONTACT LENSES   \n",
       "7998  42298563         BICYCLES   \n",
       "7999  26300286  WASHINGMACHINES   \n",
       "\n",
       "                                              main_text  \\\n",
       "0                                             WAQ284E25   \n",
       "1     LEEF IBRIDGE MOBILE SPEICHERERWEITERUNG FUER I...   \n",
       "3     HOLLANDRAD DAMEN 28 ZOLL TUSSAUD 3-GAENGE RH 5...   \n",
       "4                      DAHON SPEED D7 SCHWARZ ? FALTRAD   \n",
       "5             PNY 16GB LEGO USB FLASH DRIVE + LEGO BRIC   \n",
       "...                                                 ...   \n",
       "7995    TOPSY KINDERRAD 12 1/2 POLARWEISS O O O  419640   \n",
       "7996                     CREME ECHO SOLO 16-SPEED WHITE   \n",
       "7997  ACUVUE 1-DAY MOIST TAGESLINSEN WEICH, 30 STUEC...   \n",
       "7998          UNIVEGA TERRENO 1.0 HE MATTBLAUGRAU 45 CM   \n",
       "7999             LAVAMAT 63479 FL A+++ WASCHVOLLAUTOMAT   \n",
       "\n",
       "                                               add_text  \\\n",
       "0                                        WASCHMASCHINEN   \n",
       "1              PC__1100COMPUTINGMEMORY__1110MEMORYCARDS   \n",
       "3                         FAHRRAEDER // SPORTFAHRRAEDER   \n",
       "4                      SPORTS__30000WHEELED__30070BIKES   \n",
       "5                                   COMPONENT __ MEMORY   \n",
       "...                                                 ...   \n",
       "7995     H006W0792344__WERKZEUG_AUTO/FAHRRAD_FAHRRAEDER   \n",
       "7996                 FAHRRAEDER>>RENNRAEDER>>RENNRAEDER   \n",
       "7997  HEALTH&PERSONALCARE__3100OPTICS__3101SPHERICCO...   \n",
       "7998                                              1_7_4   \n",
       "7999                                     WASCHMASCHINEN   \n",
       "\n",
       "                   manufacturer  \n",
       "0                         BOSCH  \n",
       "1                          LEEF  \n",
       "3           SCHALOW & KROH GMBH  \n",
       "4                         DAHON  \n",
       "5                           PNY  \n",
       "...                         ...  \n",
       "7995  SI-ZWEIRAD-VERTRIEBS GMBH  \n",
       "7996                      CREME  \n",
       "7997          JOHNSON & JOHNSON  \n",
       "7998                    UNIVEGA  \n",
       "7999                        AEG  \n",
       "\n",
       "[6655 rows x 5 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the category data csv \n",
    "df = pd.read_csv('testset_C.csv', sep= ';') # data is semicolon separated\n",
    "df = df.dropna() # remove all rows with NaNs\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21151f34",
   "metadata": {},
   "source": [
    "\n",
    "First of all, we want to see whether when we train the \n",
    "data, the model does not have a specific bias towards one product group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8f3903b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('productgroup').manufacturer.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c41418",
   "metadata": {},
   "source": [
    "From the above bar graph, we can see that the number of rows that corresponds to each product group are mostly \n",
    "well distributed, meaning that we arent expecting a significant bias in the data and so any preliminary ML model trained on this should not have a problem with bias\n",
    "\n",
    "-------------------------------------------------------------\n",
    "\n",
    "When looking at the dataframe, one can see that the main_text column and the add_text column has a lot of context, in a mix of German and English words, with reference to their technical specialities (Washing machines, Bicycles, Contact Lenses and USB memory). We also see that that the main_text columns have a lot of clutter in their \n",
    "words, such as whitespaces, _, and numerical values which will not be representative  of the category. \n",
    "\n",
    "In this form, the data is not clean and inconsistent - we need to be able to simplify and find a right format for the string column to remove unnecessary dimensionality, and for the vectorizer in word2vec to find the simplest conversion of the strings that represent a productgroup category. \n",
    "\n",
    "To this end. I have done the following:\n",
    "\n",
    "- Tokenize each text column - i.e. main_text, add_text and manufacturer \n",
    "\n",
    "- Remove all alphanumeric characters  \n",
    "\n",
    "- Remove len(1) strings\n",
    "\n",
    "- Remove any integer type tokens that remain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7eff4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first tokenize the strings inside the column to make the strings more digestable\n",
    "df['main_text_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['main_text']), axis=1)\n",
    "df['add_text_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['add_text']), axis=1)\n",
    "df['manufacturer_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['manufacturer']), axis=1)\n",
    "\n",
    "# Keep alphanumerical letters, remove single length strings and \n",
    "# remove the english and german stopwords - for the column main_text, add_text and manufacturer \n",
    "df['main_text_tokenized_new'] = df.apply(lambda row: [word for word in row['main_text_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                      and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis = 1)\n",
    "df['add_text_tokenized_new'] = df.apply(lambda row: [word for word in row['add_text_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                     and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis = 1)\n",
    "df['manufacturer_tokenized_new'] = df.apply(lambda row: [word for word in row['manufacturer_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                     and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis = 1)\n",
    "\n",
    "# Encode the product group with numerical labels \n",
    "ProductLabelEncoder = LabelEncoder()\n",
    "df['productgroup'] = ProductLabelEncoder.fit_transform(df['productgroup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64cb909",
   "metadata": {},
   "source": [
    "Filtering out only the modified columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3c629178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified = df[['productgroup', 'main_text_tokenized_new','add_text_tokenized_new', 'manufacturer_tokenized_new']]\n",
    "df_modified = df_modified.dropna()\n",
    "df_modified = df_modified[df_modified['main_text_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "df_modified = df_modified[df_modified['add_text_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "df_modified = df_modified[df_modified['manufacturer_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "df_modified = df_modified.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3baa9",
   "metadata": {},
   "source": [
    "We can see that just from this data cleaning, that the columns with the most information are main_text and manufacturer, wheras the add_text does not seem to contain enough information to work with all the rows. \n",
    "\n",
    "However, for example, row 5 contains for the productgroup entry 'USB MEMORY', a add_text of [COMPONENT, MEMORY], \n",
    "which is, from an intuitive prespective, quite representative of what the USB memory is. Hence, the approach in this situation would be to merge the two columns  \n",
    "\n",
    "We will first try to see the dimensionality of the bag-of-words approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "828aad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productgroup</th>\n",
       "      <th>main_text_tokenized_new</th>\n",
       "      <th>add_text_tokenized_new</th>\n",
       "      <th>manufacturer_tokenized_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...</td>\n",
       "      <td>[FAHRRAEDER, SPORTFAHRRAEDER]</td>\n",
       "      <td>[SCHALOW, KROH, GMBH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]</td>\n",
       "      <td>[COMPONENT, MEMORY]</td>\n",
       "      <td>[PNY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[CITYBIKE, COLORS, ZOLL, INKL, KORB]</td>\n",
       "      <td>[FAHRRAEDER, FAHRRAEDER]</td>\n",
       "      <td>[TRENDMAXX, GMBH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>[AEG, WASCHMASCHINE]</td>\n",
       "      <td>[GG]</td>\n",
       "      <td>[AEG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>[WD, EXTRAKLASSE]</td>\n",
       "      <td>[WASCHTROCKNER]</td>\n",
       "      <td>[SIEMENS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>1</td>\n",
       "      <td>[DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]</td>\n",
       "      <td>[LINSEN]</td>\n",
       "      <td>[CIBA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2</td>\n",
       "      <td>[USB, MORADO, USB, MORADO]</td>\n",
       "      <td>[MEMORIAS, USB, GB]</td>\n",
       "      <td>[LEXAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>[CREME, ECHO, SOLO, WHITE]</td>\n",
       "      <td>[FAHRRAEDER, RENNRAEDER, RENNRAEDER]</td>\n",
       "      <td>[CREME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>1</td>\n",
       "      <td>[ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...</td>\n",
       "      <td>[HEALTH]</td>\n",
       "      <td>[JOHNSON, JOHNSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>3</td>\n",
       "      <td>[LAVAMAT, FL, WASCHVOLLAUTOMAT]</td>\n",
       "      <td>[WASCHMASCHINEN]</td>\n",
       "      <td>[AEG]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3680 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      productgroup                            main_text_tokenized_new  \\\n",
       "3                0  [HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...   \n",
       "5                2         [PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]   \n",
       "6                0               [CITYBIKE, COLORS, ZOLL, INKL, KORB]   \n",
       "8                3                               [AEG, WASCHMASCHINE]   \n",
       "9                3                                  [WD, EXTRAKLASSE]   \n",
       "...            ...                                                ...   \n",
       "7988             1          [DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]   \n",
       "7994             2                         [USB, MORADO, USB, MORADO]   \n",
       "7996             0                         [CREME, ECHO, SOLO, WHITE]   \n",
       "7997             1  [ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...   \n",
       "7999             3                    [LAVAMAT, FL, WASCHVOLLAUTOMAT]   \n",
       "\n",
       "                    add_text_tokenized_new manufacturer_tokenized_new  \n",
       "3            [FAHRRAEDER, SPORTFAHRRAEDER]      [SCHALOW, KROH, GMBH]  \n",
       "5                      [COMPONENT, MEMORY]                      [PNY]  \n",
       "6                 [FAHRRAEDER, FAHRRAEDER]          [TRENDMAXX, GMBH]  \n",
       "8                                     [GG]                      [AEG]  \n",
       "9                          [WASCHTROCKNER]                  [SIEMENS]  \n",
       "...                                    ...                        ...  \n",
       "7988                              [LINSEN]                     [CIBA]  \n",
       "7994                   [MEMORIAS, USB, GB]                    [LEXAR]  \n",
       "7996  [FAHRRAEDER, RENNRAEDER, RENNRAEDER]                    [CREME]  \n",
       "7997                              [HEALTH]         [JOHNSON, JOHNSON]  \n",
       "7999                      [WASCHMASCHINEN]                      [AEG]  \n",
       "\n",
       "[3680 rows x 4 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4fd2b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueTokensMainText = df_modified[\"main_text_tokenized_new\"].explode().unique()\n",
    "UniqueTokensMainText = [str(i) for i in UniqueTokensMainText]\n",
    "UniqueTokensAddText = df_modified[\"add_text_tokenized_new\"].explode().unique() \n",
    "UniqueTokensAddText = [str(i) for i in UniqueTokensAddText]\n",
    "UniqueTokensManufacturerText = df_modified[\"manufacturer_tokenized_new\"].explode().unique() \n",
    "UniqueTokensManufacturerText = [str(i) for i in UniqueTokensManufacturerText]\n",
    "\n",
    "UniqueWordsCounterMainText = lambda x: Counter([y for y in x if y in UniqueTokensMainText])\n",
    "UniqueWordsCounterAddText = lambda x: Counter([y for y in x if y in UniqueTokensAddText])\n",
    "UniqueWordsCounterManufacturerText = lambda x: Counter([y for y in x if y in UniqueTokensManufacturerText])\n",
    "\n",
    "\n",
    "df_modified['main_text_BOW'] = (pd.DataFrame(df_modified['main_text_tokenized_new'].apply(UniqueWordsCounterMainText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=UniqueTokensMainText)\n",
    "               .values\n",
    "               .tolist())\n",
    "\n",
    "df_modified['add_text_BOW'] = (pd.DataFrame(df_modified['add_text_tokenized_new'].apply(UniqueWordsCounterAddText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=UniqueTokensAddText)\n",
    "               .values\n",
    "               .tolist())\n",
    "\n",
    "df_modified['manufacturer_BOW'] = (pd.DataFrame(df_modified['manufacturer_tokenized_new'].apply(UniqueWordsCounterManufacturerText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=UniqueTokensManufacturerText)\n",
    "               .values\n",
    "               .tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c817fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productgroup</th>\n",
       "      <th>main_text_tokenized_new</th>\n",
       "      <th>add_text_tokenized_new</th>\n",
       "      <th>manufacturer_tokenized_new</th>\n",
       "      <th>main_text_BOW</th>\n",
       "      <th>add_text_BOW</th>\n",
       "      <th>manufacturer_BOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...</td>\n",
       "      <td>[FAHRRAEDER, SPORTFAHRRAEDER]</td>\n",
       "      <td>[SCHALOW, KROH, GMBH]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]</td>\n",
       "      <td>[COMPONENT, MEMORY]</td>\n",
       "      <td>[PNY]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[CITYBIKE, COLORS, ZOLL, INKL, KORB]</td>\n",
       "      <td>[FAHRRAEDER, FAHRRAEDER]</td>\n",
       "      <td>[TRENDMAXX, GMBH]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>[AEG, WASCHMASCHINE]</td>\n",
       "      <td>[GG]</td>\n",
       "      <td>[AEG]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>[WD, EXTRAKLASSE]</td>\n",
       "      <td>[WASCHTROCKNER]</td>\n",
       "      <td>[SIEMENS]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>1</td>\n",
       "      <td>[DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]</td>\n",
       "      <td>[LINSEN]</td>\n",
       "      <td>[CIBA]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2</td>\n",
       "      <td>[USB, MORADO, USB, MORADO]</td>\n",
       "      <td>[MEMORIAS, USB, GB]</td>\n",
       "      <td>[LEXAR]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>[CREME, ECHO, SOLO, WHITE]</td>\n",
       "      <td>[FAHRRAEDER, RENNRAEDER, RENNRAEDER]</td>\n",
       "      <td>[CREME]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>1</td>\n",
       "      <td>[ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...</td>\n",
       "      <td>[HEALTH]</td>\n",
       "      <td>[JOHNSON, JOHNSON]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>3</td>\n",
       "      <td>[LAVAMAT, FL, WASCHVOLLAUTOMAT]</td>\n",
       "      <td>[WASCHMASCHINEN]</td>\n",
       "      <td>[AEG]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      productgroup                            main_text_tokenized_new  \\\n",
       "3                0  [HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...   \n",
       "5                2         [PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]   \n",
       "6                0               [CITYBIKE, COLORS, ZOLL, INKL, KORB]   \n",
       "8                3                               [AEG, WASCHMASCHINE]   \n",
       "9                3                                  [WD, EXTRAKLASSE]   \n",
       "...            ...                                                ...   \n",
       "7988             1          [DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]   \n",
       "7994             2                         [USB, MORADO, USB, MORADO]   \n",
       "7996             0                         [CREME, ECHO, SOLO, WHITE]   \n",
       "7997             1  [ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...   \n",
       "7999             3                    [LAVAMAT, FL, WASCHVOLLAUTOMAT]   \n",
       "\n",
       "                    add_text_tokenized_new manufacturer_tokenized_new  \\\n",
       "3            [FAHRRAEDER, SPORTFAHRRAEDER]      [SCHALOW, KROH, GMBH]   \n",
       "5                      [COMPONENT, MEMORY]                      [PNY]   \n",
       "6                 [FAHRRAEDER, FAHRRAEDER]          [TRENDMAXX, GMBH]   \n",
       "8                                     [GG]                      [AEG]   \n",
       "9                          [WASCHTROCKNER]                  [SIEMENS]   \n",
       "...                                    ...                        ...   \n",
       "7988                              [LINSEN]                     [CIBA]   \n",
       "7994                   [MEMORIAS, USB, GB]                    [LEXAR]   \n",
       "7996  [FAHRRAEDER, RENNRAEDER, RENNRAEDER]                    [CREME]   \n",
       "7997                              [HEALTH]         [JOHNSON, JOHNSON]   \n",
       "7999                      [WASCHMASCHINEN]                      [AEG]   \n",
       "\n",
       "                                          main_text_BOW  \\\n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5     [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...   \n",
       "6     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "8     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "7988  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7994  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...   \n",
       "7996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           add_text_BOW  \\\n",
       "3     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5     [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6     [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "7988  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "7994  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7996  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       manufacturer_BOW  \n",
       "3     [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6     [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "7988  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7994  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7999  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3680 rows x 7 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6a1cdff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the bag of words approach\n",
    "df_modified = df_modified.reset_index(drop=True)\n",
    "len(df_modified['manufacturer_BOW'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e6f899bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode \n",
    "word2vec_maintext = Word2Vec([UniqueTokensMainText], min_count=1)\n",
    "# Looking at an example of the size of the vector representaiton \n",
    "v1 = word2vec_maintext.wv['HOLLANDRAD']\n",
    "np.shape(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f1036196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fa961fcb880>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_maintext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de13492",
   "metadata": {},
   "source": [
    "We can see that dimension for this feature vector are far smaller (100) for each of the tokens, which is a far more managable number. Having a look at the vocab (its a bit messy!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f262a",
   "metadata": {},
   "source": [
    "Hence, we can now encode all the columns of Main_text, Add_text, Manufacturer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "388ae242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ditto approach as we do for the main_text \n",
    "UniqueTokensAddText = df_modified[\"add_text_tokenized_new\"].explode().unique() \n",
    "UniqueTokensAddText = [str(i) for i in UniqueTokensAddText]\n",
    "UniqueTokensManufacturerText = df_modified[\"manufacturer_tokenized_new\"].explode().unique() \n",
    "UniqueTokensManufacturerText = [str(i) for i in UniqueTokensManufacturerText]\n",
    "# Train the word2vec models \n",
    "word2vec_addtext = Word2Vec([UniqueTokensAddText], min_count=1)\n",
    "word2vec_manufacturer = Word2Vec([UniqueTokensManufacturerText], min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a0e00",
   "metadata": {},
   "source": [
    "Just as a reference, we will try to see if there is any contextual learning from the word2vec model\n",
    "by searching for the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "35c27b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec_addtext.wv.most_similar('USB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "39df87fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TROCKNER', 0.3699609041213989),\n",
       " ('APPLE', 0.31504252552986145),\n",
       " ('SMART', 0.28248196840286255),\n",
       " ('CF', 0.2799317240715027),\n",
       " ('COMPUTERZUBEHÖR', 0.2574521005153656),\n",
       " ('TECHNIK', 0.25598832964897156),\n",
       " ('LDS', 0.2473604381084442),\n",
       " ('WASCHMASCHINEN', 0.24628333747386932),\n",
       " ('BRILLEN', 0.2178138792514801),\n",
       " ('MONTH', 0.20687037706375122)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "76105297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productgroup</th>\n",
       "      <th>main_text_tokenized_new</th>\n",
       "      <th>add_text_tokenized_new</th>\n",
       "      <th>manufacturer_tokenized_new</th>\n",
       "      <th>main_text_BOW</th>\n",
       "      <th>add_text_BOW</th>\n",
       "      <th>manufacturer_BOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...</td>\n",
       "      <td>[FAHRRAEDER, SPORTFAHRRAEDER]</td>\n",
       "      <td>[SCHALOW, KROH, GMBH]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]</td>\n",
       "      <td>[COMPONENT, MEMORY]</td>\n",
       "      <td>[PNY]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[CITYBIKE, COLORS, ZOLL, INKL, KORB]</td>\n",
       "      <td>[FAHRRAEDER, FAHRRAEDER]</td>\n",
       "      <td>[TRENDMAXX, GMBH]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[AEG, WASCHMASCHINE]</td>\n",
       "      <td>[GG]</td>\n",
       "      <td>[AEG]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[WD, EXTRAKLASSE]</td>\n",
       "      <td>[WASCHTROCKNER]</td>\n",
       "      <td>[SIEMENS]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>1</td>\n",
       "      <td>[DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]</td>\n",
       "      <td>[LINSEN]</td>\n",
       "      <td>[CIBA]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>2</td>\n",
       "      <td>[USB, MORADO, USB, MORADO]</td>\n",
       "      <td>[MEMORIAS, USB, GB]</td>\n",
       "      <td>[LEXAR]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>0</td>\n",
       "      <td>[CREME, ECHO, SOLO, WHITE]</td>\n",
       "      <td>[FAHRRAEDER, RENNRAEDER, RENNRAEDER]</td>\n",
       "      <td>[CREME]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>1</td>\n",
       "      <td>[ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...</td>\n",
       "      <td>[HEALTH]</td>\n",
       "      <td>[JOHNSON, JOHNSON]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>3</td>\n",
       "      <td>[LAVAMAT, FL, WASCHVOLLAUTOMAT]</td>\n",
       "      <td>[WASCHMASCHINEN]</td>\n",
       "      <td>[AEG]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      productgroup                            main_text_tokenized_new  \\\n",
       "0                0  [HOLLANDRAD, DAMEN, ZOLL, TUSSAUD, RH, cm, SCH...   \n",
       "1                2         [PNY, LEGO, USB, FLASH, DRIVE, LEGO, BRIC]   \n",
       "2                0               [CITYBIKE, COLORS, ZOLL, INKL, KORB]   \n",
       "3                3                               [AEG, WASCHMASCHINE]   \n",
       "4                3                                  [WD, EXTRAKLASSE]   \n",
       "...            ...                                                ...   \n",
       "3675             1          [DAILIES, AQUACOMFORT, PLUS, STÜCKUNISEX]   \n",
       "3676             2                         [USB, MORADO, USB, MORADO]   \n",
       "3677             0                         [CREME, ECHO, SOLO, WHITE]   \n",
       "3678             1  [ACUVUE, MOIST, TAGESLINSEN, WEICH, STUECK, BC...   \n",
       "3679             3                    [LAVAMAT, FL, WASCHVOLLAUTOMAT]   \n",
       "\n",
       "                    add_text_tokenized_new manufacturer_tokenized_new  \\\n",
       "0            [FAHRRAEDER, SPORTFAHRRAEDER]      [SCHALOW, KROH, GMBH]   \n",
       "1                      [COMPONENT, MEMORY]                      [PNY]   \n",
       "2                 [FAHRRAEDER, FAHRRAEDER]          [TRENDMAXX, GMBH]   \n",
       "3                                     [GG]                      [AEG]   \n",
       "4                          [WASCHTROCKNER]                  [SIEMENS]   \n",
       "...                                    ...                        ...   \n",
       "3675                              [LINSEN]                     [CIBA]   \n",
       "3676                   [MEMORIAS, USB, GB]                    [LEXAR]   \n",
       "3677  [FAHRRAEDER, RENNRAEDER, RENNRAEDER]                    [CREME]   \n",
       "3678                              [HEALTH]         [JOHNSON, JOHNSON]   \n",
       "3679                      [WASCHMASCHINEN]                      [AEG]   \n",
       "\n",
       "                                          main_text_BOW  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...   \n",
       "2     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...   \n",
       "3677  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3679  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           add_text_BOW  \\\n",
       "0     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3677  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3679  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       manufacturer_BOW  \n",
       "0     [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3677  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3679  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3680 rows x 7 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6d60cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified['main_text_tokenized_new'] = df_modified.apply(lambda row: [list(word2vec_maintext.wv[str(word)]) for word in row['main_text_tokenized_new']], axis = 1)\n",
    "df_modified['add_text_tokenized_new'] = df_modified.apply(lambda row: [list(word2vec_addtext.wv[str(word)]) for word in row['add_text_tokenized_new']], axis = 1)\n",
    "df_modified['manufacturer_tokenized_new'] = df_modified.apply(lambda row: [list(word2vec_manufacturer.wv[str(word)]) for word in row['manufacturer_tokenized_new']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d3c44a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productgroup</th>\n",
       "      <th>main_text_tokenized_new</th>\n",
       "      <th>add_text_tokenized_new</th>\n",
       "      <th>manufacturer_tokenized_new</th>\n",
       "      <th>main_text_BOW</th>\n",
       "      <th>add_text_BOW</th>\n",
       "      <th>manufacturer_BOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0053460123, -0.0038697799, -0.008575919, -...</td>\n",
       "      <td>[[0.0036918113, -0.0068343896, 0.009193866, -0...</td>\n",
       "      <td>[[0.0008743875, 0.0061855866, -0.0057105515, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.0044080014, -0.0096275965, 0.008749364, 0...</td>\n",
       "      <td>[[0.0018789481, 0.0044925334, 0.0014331711, -0...</td>\n",
       "      <td>[[-0.0067314343, -0.008624435, 0.008572344, -0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.0038500167, 0.006793223, -0.008262878, 0....</td>\n",
       "      <td>[[0.0036918113, -0.0068343896, 0.009193866, -0...</td>\n",
       "      <td>[[0.008779556, -0.0022710399, 0.0043710233, -0...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[0.009170933, -0.007782242, 0.0036633392, 0.0...</td>\n",
       "      <td>[[-0.001994584, -0.0051065627, -0.008482827, -...</td>\n",
       "      <td>[[-0.008404528, 0.007989359, 0.0064207194, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[[-0.0074116103, -0.0012883674, 0.009681521, -...</td>\n",
       "      <td>[[-0.00032503504, 0.0034963249, -0.006634867, ...</td>\n",
       "      <td>[[-0.0056592543, 0.0038166647, 0.0012507671, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.005104878, -0.005170582, 0.00014432408, -...</td>\n",
       "      <td>[[0.009375744, 0.0073815705, 0.0068703364, 0.0...</td>\n",
       "      <td>[[0.007947453, -0.0067654424, 0.00030367158, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.009899769, 0.0051770094, -0.00044217223, ...</td>\n",
       "      <td>[[-0.0071961125, 0.0042403643, 0.0021589922, 0...</td>\n",
       "      <td>[[-0.0029467375, 0.0020168934, -0.00722468, 0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0026367244, -0.001076646, -0.0018741011, 0...</td>\n",
       "      <td>[[0.0036918113, -0.0068343896, 0.009193866, -0...</td>\n",
       "      <td>[[0.0021397038, 0.0018443122, -0.009292459, 0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>1</td>\n",
       "      <td>[[0.002370208, 0.0038233954, 0.008750742, 0.00...</td>\n",
       "      <td>[[0.00095079414, 0.008600361, -0.0040079374, 0...</td>\n",
       "      <td>[[0.0006473395, 0.006498289, 0.0058561303, -0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>3</td>\n",
       "      <td>[[-0.0052374434, -0.0098278, 0.0073967576, 0.0...</td>\n",
       "      <td>[[-0.00062350184, 0.0014439351, -0.0011971698,...</td>\n",
       "      <td>[[-0.008404528, 0.007989359, 0.0064207194, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      productgroup                            main_text_tokenized_new  \\\n",
       "0                0  [[0.0053460123, -0.0038697799, -0.008575919, -...   \n",
       "1                2  [[-0.0044080014, -0.0096275965, 0.008749364, 0...   \n",
       "2                0  [[-0.0038500167, 0.006793223, -0.008262878, 0....   \n",
       "3                3  [[0.009170933, -0.007782242, 0.0036633392, 0.0...   \n",
       "4                3  [[-0.0074116103, -0.0012883674, 0.009681521, -...   \n",
       "...            ...                                                ...   \n",
       "3675             1  [[-0.005104878, -0.005170582, 0.00014432408, -...   \n",
       "3676             2  [[-0.009899769, 0.0051770094, -0.00044217223, ...   \n",
       "3677             0  [[0.0026367244, -0.001076646, -0.0018741011, 0...   \n",
       "3678             1  [[0.002370208, 0.0038233954, 0.008750742, 0.00...   \n",
       "3679             3  [[-0.0052374434, -0.0098278, 0.0073967576, 0.0...   \n",
       "\n",
       "                                 add_text_tokenized_new  \\\n",
       "0     [[0.0036918113, -0.0068343896, 0.009193866, -0...   \n",
       "1     [[0.0018789481, 0.0044925334, 0.0014331711, -0...   \n",
       "2     [[0.0036918113, -0.0068343896, 0.009193866, -0...   \n",
       "3     [[-0.001994584, -0.0051065627, -0.008482827, -...   \n",
       "4     [[-0.00032503504, 0.0034963249, -0.006634867, ...   \n",
       "...                                                 ...   \n",
       "3675  [[0.009375744, 0.0073815705, 0.0068703364, 0.0...   \n",
       "3676  [[-0.0071961125, 0.0042403643, 0.0021589922, 0...   \n",
       "3677  [[0.0036918113, -0.0068343896, 0.009193866, -0...   \n",
       "3678  [[0.00095079414, 0.008600361, -0.0040079374, 0...   \n",
       "3679  [[-0.00062350184, 0.0014439351, -0.0011971698,...   \n",
       "\n",
       "                             manufacturer_tokenized_new  \\\n",
       "0     [[0.0008743875, 0.0061855866, -0.0057105515, 0...   \n",
       "1     [[-0.0067314343, -0.008624435, 0.008572344, -0...   \n",
       "2     [[0.008779556, -0.0022710399, 0.0043710233, -0...   \n",
       "3     [[-0.008404528, 0.007989359, 0.0064207194, 0.0...   \n",
       "4     [[-0.0056592543, 0.0038166647, 0.0012507671, -...   \n",
       "...                                                 ...   \n",
       "3675  [[0.007947453, -0.0067654424, 0.00030367158, 0...   \n",
       "3676  [[-0.0029467375, 0.0020168934, -0.00722468, 0....   \n",
       "3677  [[0.0021397038, 0.0018443122, -0.009292459, 0....   \n",
       "3678  [[0.0006473395, 0.006498289, 0.0058561303, -0....   \n",
       "3679  [[-0.008404528, 0.007989359, 0.0064207194, 0.0...   \n",
       "\n",
       "                                          main_text_BOW  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, ...   \n",
       "2     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...   \n",
       "3677  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3679  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           add_text_BOW  \\\n",
       "0     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3677  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3679  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       manufacturer_BOW  \n",
       "0     [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "3675  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3677  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3679  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3680 rows x 7 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d686af3",
   "metadata": {},
   "source": [
    "Problem: The approach with the word2vec does not produce consistent size numerical vectors - list sizes of 100 \n",
    "         for each word may be small, but produces inconsistent length 100 X N vectors which cannot be read. Hence, \n",
    "          at this point, I have moved to working with the bag of words approach, meaning that I will be using the \n",
    "          main_text_BOW, add_text_BOW and manufacturer_BOW columns \n",
    "          \n",
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "Now we have a numerical representation of each token in the main text, add text and manufacturer\n",
    "\n",
    "Hence, now we will have to determine what methodology is best to train this data? \n",
    "\n",
    "For this I have chosen the Random Forest method. This method was chosen because amongst others, there are two main reasons:\n",
    "\n",
    "1. When training, the Random Forest method is robust to the predictor types - in this case, we may \n",
    "   want to find out what type of Washingmachine it is as a subcategory later with addiitonal numerical categories, \n",
    "   which is something that the random forest model can support easily.\n",
    "   \n",
    "   \n",
    "   \n",
    "2. Whilst being comparably accurate compared to deep learning methods, it is also far more interpretable. Also the \n",
    "   ability to ascertain which class/numerical feature is the most important in the prediction can come in useful \n",
    "   when wanting to reduce unnecessary dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "70f68650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-526-31264b135fdf>:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rnd_clf.fit(X_train_new, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37907608695652173"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_manufac, test_manufac = train_test_split(df_modified[['manufacturer_BOW', 'productgroup']], test_size=0.2)\n",
    "X_train, y_train = train_manufac[['manufacturer_BOW']], train[['productgroup']]\n",
    "X_test, y_test = test_manufac[['manufacturer_BOW']], test[['productgroup']]\n",
    "X_train_new = [] \n",
    "for i in np.array(X_train['manufacturer_BOW']):\n",
    "    X_train_new.append(i)\n",
    "X_test_new = [] \n",
    "for i in np.array(X_test['manufacturer_BOW']): \n",
    "    X_test_new.append(i)\n",
    "    \n",
    "rnd_clf = RandomForestClassifier(n_estimators=900, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train_new, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test_new)\n",
    "accuracy_score(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87139d76",
   "metadata": {},
   "source": [
    "Training the Random Forest Model based on the add text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "7a5cee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-527-3f4de570e899>:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rnd_clf.fit(X_train_new, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.907608695652174"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_addtext, test_addtext = train_test_split(df_modified[['add_text_BOW', 'productgroup']], test_size=0.2)\n",
    "X_train, y_train = train_addtext[['add_text_BOW']], train_addtext[['productgroup']]\n",
    "X_test, y_test = test_addtext[['add_text_BOW']], test_addtext[['productgroup']]\n",
    "X_train_new = [] \n",
    "\n",
    "for i in np.array(X_train['add_text_BOW']):\n",
    "    X_train_new.append(i)\n",
    "X_test_new = [] \n",
    "for i in np.array(X_test['add_text_BOW']): \n",
    "    X_test_new.append(i)\n",
    "    \n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train_new, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test_new)\n",
    "accuracy_score(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d580b",
   "metadata": {},
   "source": [
    "Training the Random Forest Model based on the main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a4f4523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-528-29cdf23c3d72>:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rnd_clf.fit(X_train_new, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9198369565217391"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_maintext, test_maintext = train_test_split(df_modified[['main_text_BOW', 'productgroup']], test_size=0.2)\n",
    "X_train, y_train = train_maintext[['main_text_BOW']], train_maintext[['productgroup']]\n",
    "X_test, y_test = test_maintext[['main_text_BOW']], test_maintext[['productgroup']]\n",
    "X_train_new = [] \n",
    "\n",
    "for i in np.array(X_train['main_text_BOW']):\n",
    "    X_train_new.append(i)\n",
    "X_test_new = [] \n",
    "for i in np.array(X_test['main_text_BOW']): \n",
    "    X_test_new.append(i)\n",
    "    \n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train_new, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test_new)\n",
    "accuracy_score(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d58e9",
   "metadata": {},
   "source": [
    "Finally, we can now package this into a class, which we may use to analyze \n",
    "the data and use with a microservice api.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2a13de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GFKTaskMLModelGenerator(): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, InputCSV, N_Estimator, N_leaf_nodes, test_size, feature_column):\n",
    "        \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "        \n",
    "        Constructor for our class\n",
    "        \n",
    "        InputCSV  - the path to the csv with the relevant columns \n",
    "        N_Estimator - number of estimators we want to use with the random forest model \n",
    "        leaf_nodes - number of leaf nodes we use with the random forest model \n",
    "        test_size - the proportion of the data we want to split when splitting into training and validati ondata\n",
    "        feature_column - which feature column to use as the feature when training the ML model \n",
    "        \n",
    "        \"\"\"\n",
    "        self._InputCSV = InputCSV # Path to CSV to read in the product \n",
    "        self._N_Estimator =  N_Estimator # Number of estimates for the random forest classifier \n",
    "        self._leaf_nodes =  N_leaf_nodes\n",
    "        self._test_size = test_size \n",
    "        self._feature_column = feature_column\n",
    "        \n",
    "        # Dictionary to translate the feature column \n",
    "        self._BOWDict = {\n",
    "                'main_text':'main_text_BOW',\n",
    "                'add_text':'add_text_BOW',\n",
    "                'manufacturer':'manufacturer_BOW'\n",
    "        }\n",
    "        \n",
    "    def CleanTextColumns(self):\n",
    "        \"\"\"\n",
    "        Description: \n",
    "        -----------\n",
    "        \"\"\"\n",
    "        self._ProductLabelEncoder = LabelEncoder() # LabelEnocoder - will need later to inverse label change\n",
    "        df = pd.read_csv(self._InputCSV, sep= ';') # data is semicolon separated\n",
    "        df = df.dropna() # remove all rows with NaNs\n",
    "        \n",
    "        # We first tokenize the strings inside the column to make the strings more digestable\n",
    "        df['main_text_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['main_text']), axis=1)\n",
    "        df['add_text_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['add_text']), axis=1)\n",
    "        df['manufacturer_tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['manufacturer']), axis=1)\n",
    "        \n",
    "        # Remove the english and german stopwords - for the column main_text\n",
    "        df['main_text_tokenized_new'] = df.apply(lambda row: [word for word in row['main_text_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                          and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis =1)\n",
    "        df['add_text_tokenized_new'] = df.apply(lambda row: [word for word in row['add_text_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                         and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis =1)\n",
    "        df['manufacturer_tokenized_new'] = df.apply(lambda row: [word for word in row['manufacturer_tokenized'] if word.isalnum() and len(word) != 1 \n",
    "                                                         and word not in stopEnglish and word not in stopGerman and not any(c.isdigit() for c in word)], axis =1)\n",
    "    \n",
    "        self._df_modified = df[['productgroup', 'main_text_tokenized_new','add_text_tokenized_new', 'manufacturer_tokenized_new']]\n",
    "        self._df_modified['productgroup'] = self._ProductLabelEncoder.fit_transform(self._df_modified['productgroup'])\n",
    "        self._df_modified = self._df_modified[self._df_modified['main_text_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "        self._df_modified = self._df_modified[self._df_modified['add_text_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "        self._df_modified = self._df_modified[self._df_modified['manufacturer_tokenized_new'].map(lambda d: len(d)) > 0]\n",
    "        self._df_modified = self._df_modified.reindex()    \n",
    "    \n",
    "    def MakeWord2Vec(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "        \n",
    "        Makes a Word2Vec list of the string vectorizations \n",
    "        \n",
    "        \"\"\"\n",
    "        # Extract the unique tokens in each of the feature columns \n",
    "        UniqueTokensMainText = self._df_modified[\"main_text_tokenized_new\"].explode().unique()\n",
    "        UniqueTokensMainText = [str(i) for i in UniqueTokensMainText]\n",
    "        UniqueTokensAddText = self._df_modified[\"add_text_tokenized_new\"].explode().unique() \n",
    "        UniqueTokensAddText = [str(i) for i in UniqueTokensAddText]\n",
    "        UniqueTokensManufacturerText = self._df_modified[\"manufacturer_tokenized_new\"].explode().unique() \n",
    "        UniqueTokensManufacturerText = [str(i) for i in UniqueTokensManufacturerText]\n",
    "        \n",
    "        # Train the word2Vec models \n",
    "        word2vec_maintext = Word2Vec([UniqueTokensMainText], min_count=1) # word2vec for maintext\n",
    "        word2vec_addtext = Word2Vec([UniqueTokensMainText], min_count=1) # word2vec for addtext  \n",
    "        word2vec_manufacturer = Word2Vec([UniqueTokensManufacturerText], min_count=1) # word2vec for Manufacturer\n",
    "        \n",
    "        # Relabel the tokens as feature vectors as taken from the word2vec models \n",
    "        self._df_modified['main_text_tokenized_new'] = self._df_modified.apply(lambda row: [word2vec_maintext.wv[str(word)] for word in row['main_text_tokenized_new']], axis = 1)\n",
    "        self._df_modified['add_text_tokenized_new'] = self._df_modified.apply(lambda row: [word2vec_addtext.wv[str(word)] for word in row['add_text_tokenized_new']], axis = 1)\n",
    "        self._df_modified['manufacturer_tokenized_new'] = self._df_modified.apply(lambda row: [word2vec_manufacturer.wv[str(word)] for word in row['manufacturer_tokenized_new']], axis = 1)\n",
    "    \n",
    "    def MakeOneHot(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "        \n",
    "        Makes a one-hot implementation of the text data columns \n",
    "        \n",
    "        \"\"\"\n",
    "        # Extract the unique tokens in each of the feature columns \n",
    "        \n",
    "        # Main Text\n",
    "        self._UniqueTokensMainText = self._df_modified[\"main_text_tokenized_new\"].explode().unique()\n",
    "        self._UniqueTokensMainText = [str(i) for i in self._UniqueTokensMainText]\n",
    "        \n",
    "        # Add Text \n",
    "        self._UniqueTokensAddText = self._df_modified[\"add_text_tokenized_new\"].explode().unique() \n",
    "        self._UniqueTokensAddText = [str(i) for i in self._UniqueTokensAddText]\n",
    "        \n",
    "        # Manufacturer Text \n",
    "        self._UniqueTokensManufacturerText = self._df_modified[\"manufacturer_tokenized_new\"].explode().unique() \n",
    "        self._UniqueTokensManufacturerText = [str(i) for i in self._UniqueTokensManufacturerText]\n",
    "\n",
    "        # Train the Bag of Words models \n",
    "        self._UniqueWordsCounterMainText = lambda x: Counter([y for y in x if y in self._UniqueTokensMainText])\n",
    "        self._UniqueWordsCounterAddText = lambda x: Counter([y for y in x if y in self._UniqueTokensAddText])\n",
    "        self._UniqueWordsCounterManufacturerText = lambda x: Counter([y for y in x if y in self._UniqueTokensManufacturerText])\n",
    "        \n",
    "        # Create the bag of words columns for the main_text\n",
    "        self._df_modified['main_text_BOW'] = (pd.DataFrame(self._df_modified['main_text_tokenized_new'].apply(self._UniqueWordsCounterMainText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=self._UniqueTokensMainText)\n",
    "               .values\n",
    "               .tolist())\n",
    "        \n",
    "        # Create the bag of words columns for the add_text \n",
    "        self._df_modified['add_text_BOW'] = (pd.DataFrame(self._df_modified['add_text_tokenized_new'].apply(self._UniqueWordsCounterAddText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=self._UniqueTokensAddText)\n",
    "               .values\n",
    "               .tolist())\n",
    "        \n",
    "        # Create the bag of words columns for the manufacturer \n",
    "        self._df_modified['manufacturer_BOW'] = (pd.DataFrame(self._df_modified['manufacturer_tokenized_new'].apply(self._UniqueWordsCounterManufacturerText).values.tolist())\n",
    "               .fillna(0)\n",
    "               .astype(int)\n",
    "               .reindex(columns=self._UniqueTokensManufacturerText)\n",
    "               .values\n",
    "               .tolist())\n",
    "         \n",
    "    def TrainMLModel(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "                     \n",
    "        Takes the one-hot inputs and the categories and splits the data into a training \n",
    "        and validation set, then trains the random forest model. \n",
    "     \n",
    "        \"\"\"\n",
    "        # Split the test into a training and a validation set \n",
    "        train, val = train_test_split(self._df_modified[['productgroup', self._BOWDict[self._feature_column]]], \n",
    "                                       test_size = 0.3)\n",
    "        # We take the manufacturer BOW columns as the training data\n",
    "        X_train, y_train = train[[self._BOWDict[self._feature_column]]], train[['productgroup']]\n",
    "        X_test, y_test = val[[self._BOWDict[self._feature_column]]], val[['productgroup']]\n",
    "        X_train_new, X_test_new = [], []\n",
    "        \n",
    "        # Reformat the one-hot data so that it can be read by the classifier\n",
    "        for entry in np.array(X_train[self._BOWDict[self._feature_column]]):\n",
    "            X_train_new.append(entry)\n",
    "            \n",
    "        # Ditto with the test data \n",
    "        for entry in np.array(X_test[self._BOWDict[self._feature_column]]): \n",
    "            X_test_new.append(entry)\n",
    "        \n",
    "        # Train the Random Forest Model \n",
    "        self._rf_clf = RandomForestClassifier(n_estimators=self._N_Estimator, max_leaf_nodes=self._leaf_nodes)\n",
    "        self._rf_clf.fit(X_train_new, y_train)\n",
    "        \n",
    "    def InputText(self, string):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        \n",
    "        Takes a custom input, tokenizes it, and returns the one-hot vectorized \n",
    "        format of that one-hot vector that can be trained with the random forest model.\n",
    "         \n",
    "        The modification of the string has to follow the same formatting as has been done\n",
    "        for the sample columns. \n",
    "        \n",
    "        \"\"\"\n",
    "        # NLTK tokenize\n",
    "        tokenized_string = nltk.word_tokenize(string)\n",
    "        \n",
    "        # Modify string in the same way as  CleanTextColumn function \n",
    "        modified_tokenized_string = [word for word in tokenized_string if word.isalnum() and len(word) != 1 \n",
    "                                     and word not in stopEnglish and word not in stopGerman \n",
    "                                     and not any(c.isdigit() for c in word)]\n",
    "        \n",
    "        # From the relevant category type and counter, change the modified_tokenized string so that \n",
    "        # we get a one-hot format that can be read by the trained ML model. \n",
    "        OneHotString = []\n",
    "        Tokens = None \n",
    "        # Select the correct tokens \n",
    "        if self._feature_column == 'main_text':\n",
    "            Tokens = self._UniqueTokensMainText\n",
    "        if self._feature_column == 'add_text':\n",
    "            Tokens = self._UniqueTokensAddText\n",
    "        if self._feature_column == 'manufacturer': \n",
    "            Tokens = self.UniqueTokensManufacturerText\n",
    "        \n",
    "        # Loop over Tokens of columns \n",
    "        for onehotentry in Tokens:\n",
    "            Counter = 0 # Counter to create the one-hot vector as needed \n",
    "            # Loop over the tokenized string \n",
    "            for entry in modified_tokenized_string:\n",
    "                if onehotentry == entry: # if we have a matching string, then we add a counter\n",
    "                    Counter += 1 \n",
    "            OneHotString.append(Counter) # append as an element to the one-hot vector \n",
    "        \n",
    "        OneHotStringOutput = [entry for entry in OneHotString]\n",
    "        return [OneHotString]\n",
    "    \n",
    "    def PredictCategory(self, string):\n",
    "        \"\"\"\n",
    "        \n",
    "        Function that takes the string, and then returns the category \n",
    "        from the trained model produced from the class \n",
    "        \n",
    "        \"\"\"\n",
    "        # Predict the category for the string input  \n",
    "        Cat = self._rf_clf.predict(self.InputText(string)) # Predict the category based on the input string \n",
    "        CategoryOutput = self._ProductLabelEncoder.inverse_transform(Cat)[0] # Inverse transform numerical back to string \n",
    "        return CategoryOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195a736",
   "metadata": {},
   "source": [
    "Using the class above, we follow the following pipeline:\n",
    "\n",
    "1. Read the CSV, define the Random Forest variables \n",
    "\n",
    "2. Tokenize, edit and reduce the dimensionality of the feature data \n",
    "\n",
    "3. Convert the tokenzed string data into one-hot representation\n",
    "\n",
    "4. Finally train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8b85ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-195-ea8fa3953aa0>:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._df_modified['productgroup'] = self._ProductLabelEncoder.fit_transform(self._df_modified['productgroup'])\n",
      "<ipython-input-195-ea8fa3953aa0>:165: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._rf_clf.fit(X_train_new, y_train)\n"
     ]
    }
   ],
   "source": [
    "Model = GFKTaskMLModelGenerator('testset_C.csv', 10, 16, 0.3, 'main_text')\n",
    "Model.CleanTextColumns()\n",
    "Model.MakeOneHot()\n",
    "Model.TrainMLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5b8f40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USB MEMORY'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For example, getting the category for \n",
    "the string input 'LEEF IBRIDGE MOBILE SPEICHERERWEITERUNG FUER IPHONE, IPAD UND IPOD - MIT LIGHTNING UND USB, 128 GB'\n",
    "\"\"\"\n",
    "ExampleInput = 'LEEF IBRIDGE MOBILE SPEICHERERWEITERUNG FUER IPHONE, IPAD UND IPOD - MIT LIGHTNING UND USB, 128 GB'\n",
    "Model.PredictCategory(ExampleInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd4f23",
   "metadata": {},
   "source": [
    "Hence, we have reproduced the category for this string~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
